{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCmdocFibGxtQ/1yuqtQC0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyzira/Train-and-Run-LLama3.1-on-Colab/blob/main/Load_and_Use_your_trained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the second part to your trained model\n",
        "\n",
        "If you have already created the gguf and the Modelfile you can skip these steps!"
      ],
      "metadata": {
        "id": "FCsin0CCRcPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "U56XNcxvRnrx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU5vYNzXRWNE",
        "outputId": "db18a7c1-26e6-45db-936f-89fabc0fb1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import os\n",
        "\n",
        "# 1. Definiere die Pfade\n",
        "# ERSETZE DIES DURCH DEN PFAD ZU DEINEM GESPEICHERTEN LORA-ORDNER\n",
        "lora_model_path = \"/content/drive/My Drive/lora_model\"\n",
        "\n",
        "gguf_output_directory = \"/content/drive/My Drive/my_trained_llama\"\n",
        "os.makedirs(gguf_output_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "# 2. Lade das Modell und die LoRA-Gewichte\n",
        "# Beim Laden für die GGUF-Konvertierung MUSST du load_in_4bit auf False setzen.\n",
        "# model_name sollte dein LORA-Ordnerpfad sein. Unsloth lädt automatisch das Basismodell und mergt die LoRA-Gewichte.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = lora_model_path, # Pfad zu deinen LoRA-Adaptern\n",
        "    max_seq_length = 8192,         # Verwende die max_seq_length, die du beim Training verwendet hast\n",
        "    dtype = None,                  # Unsloth wählt automatisch die beste dtype\n",
        "    load_in_4bit = False,          # Wichtig: Muss False sein für die Konvertierung\n",
        ")\n",
        "\n",
        "# 3. Konvertierung nach GGUF\n",
        "# 'q4_k_m' ist empfohlen für Ollama (gute Balance zwischen Größe und Qualität)\n",
        "model.save_pretrained_gguf(\n",
        "    gguf_output_directory,\n",
        "    tokenizer,\n",
        "    quantization_method = \"q4_k_m\"\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ GGUF-Konvertierung abgeschlossen! Die Datei(en) befinden sich im Ordner: {gguf_output_directory}\")\n",
        "print(f\"Die GGUF-Datei hat den Namen: {gguf_output_directory}/model-unsloth-Q4_K_M.gguf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "Vrx5xMHtR2rS",
        "outputId": "173bbeb0-1ab3-45ce-9ec0-374ccc91ee9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth cannot find any torch accelerators? You need a GPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4174019595.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Definiere die Pfade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA, AMD and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accelerator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth cannot find any torch accelerators? You need a GPU.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0maccelerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccelerator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth cannot find any torch accelerators? You need a GPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelfile erstellen\n",
        "import os\n",
        "\n",
        "modelfile_content = \"\"\"# Modelfile\n",
        "FROM /content/drive/My Drive/my_trained_llama/model-unsloth-Q4_K_M.gguf\n",
        "TEMPLATE \\\"\\\"\\\"{% if system %}<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ system }}<|eot_id|>{% endif %}<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\\\"\\\"\\\"\n",
        "STOP \"<|eot_id|>\"\n",
        "STOP \"<|end_of_text|>\"\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(gguf_output_directory, \"Modelfile\"), \"w\") as f:\n",
        "    f.write(modelfile_content)\n",
        "\n",
        "print(\"✅ Modelfile erstellt.\")"
      ],
      "metadata": {
        "id": "NsKo2CcpSjig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you already have Created a gguf and Modelfile continue from here"
      ],
      "metadata": {
        "id": "Y_ZE1zA_TDsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ollama installieren\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Ollama-Dienst im Hintergrund starten\n",
        "import subprocess\n",
        "import time\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "print(\"\\n✅ Ollama Service gestartet.\")"
      ],
      "metadata": {
        "id": "ax40jEUWS_24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importiere das Modell in Ollama.\n",
        "# \"mein-llama-31\" ist der Name, den du deinem Modell gibst.\n",
        "!ollama create mein-llama-31 -f \"/content/drive/My Drive/my_trained_llama/Modelfile\"\n",
        "\n",
        "print(\"\\n🎉 Modell erfolgreich in Ollama importiert!\")"
      ],
      "metadata": {
        "id": "fZtzrHW4S78Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run mein-llama-31 \"Was ist das Wichtigste, das ich über das Finetuning mit LoRA wissen sollte?\""
      ],
      "metadata": {
        "id": "_eBTLapuT1j2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}